const fs = require('fs'); // Check if a file path was provided const checkFilePath = (filePath) => { if (!fs.existsSync(filePath)) { console.log("The file does not exist."); process.exit(1); } }; // Function to read a file asynchronously const readFileAsync = (filePath) => { return new Promise((resolve, reject) => { fs.readFile(filePath, { encoding: 'utf-8' }, (err, data) => { if (err) { reject(err); } else { resolve(data); } }); }); }; function writeFileAsync(filePath, data) { return new Promise((resolve, reject) => {fs.writeFile(filePath, data, (err) => {if (err) { reject(err); } else { resolve(); } })})}; const getUrlTypeAndForm = (urlJSON) => { let urltype, form; if (urlJSON.download.length > 0) { urltype = "download"; form = urlJSON.download; } else if (urlJSON.upload.length > 0) { urltype = "upload"; form = urlJSON.upload; } else if (urlJSON.load.length > 0) { urltype = "load"; form = urlJSON.load; } else if (urlJSON.unload.length > 0) { urltype = "unload"; form = urlJSON.unload; } return { urltype, form }; }; // Read the command line arguments const filePath = process.argv[2]; const urlPath = process.argv[3]; // Check if file paths were provided if (process.argv.length < 4) { console.log("Usage: node netlog.js <path_to_netlog_file> <path_to_urls_file>"); process.exit(1); } // Check if the files exist checkFilePath(filePath); checkFilePath(urlPath); // Function to parse JSON from a file const parseJSONFromFile = (filePath) => { const fileContent = fs.readFileSync(filePath); return JSON.parse(fileContent); }; // Read the content of the Netlog file (async () => { try { const data = await readFileAsync(filePath); const events = data.split('\n').slice(2); // Read the URLs from the file const urlJSON = parseJSONFromFile(urlPath); const urlTypeAndForm = getUrlTypeAndForm(urlJSON); const { urltype, form } = urlTypeAndForm; console.log(urltype); const byte_time_list = []; const results = []; events.forEach((element, index) => { if (element.trim() === "") { return; } if (element.slice(-2) === ']}') { eachEvent = element.slice(0, -2); } else { eachEvent = element.slice(0, -1); } try { const eventData = JSON.parse(eachEvent); // Check if the eventData meets certain conditions if ( eventData.hasOwnProperty('params') && typeof (eventData.params) === 'object' && eventData.params.hasOwnProperty('url') && form.some(url => eventData.params.url.includes(url) && eventData.type === 2 ) ) { if (eventData.source.hasOwnProperty('id')) { const id = eventData.source; results.push({ sourceID: id, index: index, dict: eventData }); } } if ( results.some(result => result.sourceID && result.sourceID.id === eventData.source.id) ) { if ((eventData.type === 123 || eventData.type === 122) && eventData.hasOwnProperty('params') && (urltype == "download") && eventData.params.hasOwnProperty('byte_count')) { let existingIdIndex = byte_time_list.findIndex(item => item.id === eventData.source.id); if (existingIdIndex === -1) { // If id does not exist, add it to byte_time_list byte_time_list.push({ id: eventData.source.id, type: urltype, progress: [] }); existingIdIndex = byte_time_list.length - 1; } // Push new progress values byte_time_list[existingIdIndex].progress.push({ bytecount: eventData.params.byte_count, time: eventData.time }); //type 160 if (eventData.params.hasOwnProperty('source_dependency') && eventData.type === 160) { console.log(eventData.params.source_dependency.id); } } else if (eventData.type === 450 && eventData.hasOwnProperty('params') && urltype == "upload" && eventData.params.hasOwnProperty('current_position')) { let existingIdIndex = byte_time_list.findIndex(item => item.id === eventData.source.id); if (existingIdIndex === -1) { // If id does not exist, add it to byte_time_list byte_time_list.push({ id: eventData.source.id, type: urltype, progress: [] }); existingIdIndex = byte_time_list.length - 1; } // Push new progress values byte_time_list[existingIdIndex].progress.push({ current_position: eventData.params.current_position, time: eventData.time }); } } } catch (error) { console.error("Error parsing line", index, ":", error); } }); // Write byte_time_list to file await writeFileAsync('byte_time_list.json', JSON.stringify(byte_time_list, null, 2)); } catch (error) { console.error("Error reading the file:", error); } })(); // You can continue with your fs.readFile logic here (async () => { http_stream_job_ids=[]; try { checkFilePath(filePath); const data = await readFileAsync(filePath); const events = data.split('\n').slice(2); // Check if the file exists checkFilePath(urlPath); // Read the URLs from the file const urlJSON = parseJSONFromFile(urlPath); // Read the content of the byte_time_list.json file const byteTimeList = parseJSONFromFile("byte_time_list.json"); checkFilePath("byte_time_list.json"); // Populate the list array const list = byteTimeList.map(item => item.id); console.log(list); const urlTypeAndForm = getUrlTypeAndForm(urlJSON); const { urltype, form } = urlTypeAndForm; console.log(urltype); events.forEach((element, index) => { if (element.trim() === "") { return; } if (element.slice(-2) === ']}') { eachEvent = element.slice(0, -2); } else { eachEvent = element.slice(0, -1); } const eventData = JSON.parse(eachEvent); if (eventData.hasOwnProperty('params') && eventData.params.hasOwnProperty('source_dependency') && eventData.type === 160) { // console.log(list) if (list.includes(eventData.source.id)) { console.log("source_dependency id:", eventData.params.source_dependency.id); http_stream_job_ids.push([eventData.source.id, eventData.params.source_dependency.id]); } } }); await writeFileAsync('httpStreamJobIds.txt', http_stream_job_ids.join('\n')); } catch (error) { console.error("Error reading the file:", error); } })(); (async () => { socketIds = [] try { const data = await readFileAsync(filePath); const events = data.split('\n').slice(2); // Read the URLs from the file // Read the content of the byte_time_list.json file await checkFilePath("httpStreamJobIds.txt"); httpstreamJobs = await readFileAsync("httpStreamJobIds.txt") // const httpstreamList = httpstreamJobs.split('\n'); if (httpstreamJobs.length !== 0) { //currently each item in the list has a comma, spli each of them and only taee the second value httpstreamList = httpstreamJobs.split('\n').map(item => item.split(',')[1]); events.forEach((element, index) => { if (element.trim() === "") { return; } if (element.slice(-2) === ']}') { eachEvent = element.slice(0, -2); } else { eachEvent = element.slice(0, -1); } const eventData = JSON.parse(eachEvent); if (eventData.hasOwnProperty('params') && eventData.params.hasOwnProperty('source_dependency') && eventData.type === 108) { if (httpstreamList.includes((eventData.source.id))) { console.log("source_dependency id part 2:", eventData.params.source_dependency.id); socketIds.push(eventData.params.source_dependency.id); } } }); } await writeFileAsync('socketIds.txt', socketIds.join('\n')); } catch (error) { console.error("Error reading the file:", error); } })(); (async () => { try { const data = await readFileAsync(filePath); const events = data.split('\n').slice(2); // Read the URLs from the file // Read the content of the byte_time_list.json file await checkFilePath("socketIds.txt"); socketdata = await readFileAsync("socketIds.txt") // const httpstreamList = httpstreamJobs.split('\n'); if (socketdata.length !== 0) { socketDataList = socketdata.split('\n').map(Number); events.forEach((element, index) => { if (element.trim() === "") { return; } if (element.slice(-2) === ']}') { eachEvent = element.slice(0, -2); } else { eachEvent = element.slice(0, -1); } const eventData = JSON.parse(eachEvent); if (eventData.hasOwnProperty('params') && eventData.params.hasOwnProperty('byte_count')) { if (socketDataList.includes((eventData.source.id))) { console.log("byte_count:", eventData.params.byte_count); // console.log("source_dependency id part 2:", eventData.params.source_dependency.id); } } }); } } catch (error) { console.error("Error reading the file:", error); } })();
